{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax.random as jr\n",
    "import optax\n",
    "\n",
    "import jax.numpy as np\n",
    "import cma\n",
    "import pandas as pd\n",
    "from memo import memo\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from enum import IntEnum, auto\n",
    "# read in files\n",
    "# Read UK_df.csv as pandas dataframe\n",
    "original_UK_dialogue = pd.read_csv('UK_df.csv')\n",
    "original_UK_politeness = pd.read_csv('UK_direct_df.csv')\n",
    "original_UK_narrator = pd.read_csv('UK_narrator_df.csv')\n",
    "original_US_dialogue = pd.read_csv('US_df.csv')\n",
    "original_US_politeness = pd.read_csv('US_direct_df.csv')\n",
    "original_US_narrator = pd.read_csv('US_narrator_df.csv')\n",
    "dataframes = [original_UK_dialogue, original_UK_politeness, original_UK_narrator, original_US_dialogue, original_US_politeness, original_US_narrator]\n",
    "def elim_outliers(df):\n",
    "    # dropped Unnamed: 0 column\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    filtered_df = df.loc[(df['response'] > 95) | (df['response'] < 5)]\n",
    "    for id in df['person_id'].unique():\n",
    "        if len(filtered_df[filtered_df['person_id'] == id])/len(df[df['person_id'] == id])>0.8:\n",
    "            df.drop(df[df['person_id'] == id].index, inplace=True)\n",
    "    df['predicate Z-score'] = df.groupby(['person_id','predicate'])['response'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    # if has_intensifier = no then change 'intensifier' to 'none'\n",
    "    df.loc[df['has intensifier?'] == 'no', 'intensifier'] = 'none'\n",
    "    return df\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = elim_outliers(dataframes[i])\n",
    "dialogue = pd.concat([dataframes[0], dataframes[3]])\n",
    "politeness = pd.concat([dataframes[1], dataframes[4]])\n",
    "UK_dialogue = dataframes[0]\n",
    "US_dialogue = dataframes[3]\n",
    "UK_politeness = dataframes[1]\n",
    "US_politeness = dataframes[4]\n",
    "\n",
    "# end of reading in data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# compute U_soc (social Utility)\n",
    "U_soc_data = politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "UK_U_soc_data = UK_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "US_U_soc_data = US_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "class W(IntEnum):  # utterance space\n",
    "    # intensifiers\n",
    "    none = auto(0)\n",
    "    slightly= auto()\n",
    "    kind_of = auto()\n",
    "    quite = auto()\n",
    "    very= auto()\n",
    "    extremely= auto()\n",
    "\n",
    "class P(IntEnum):\n",
    "    # predicates\n",
    "    boring = auto(0)\n",
    "    concerned = auto()\n",
    "    difficult = auto()\n",
    "    exhausted = auto()\n",
    "    helpful = auto()\n",
    "    impressive = auto()\n",
    "    understandable = auto()\n",
    "epsilon = 0.01\n",
    "infty = 10000000\n",
    "utterences =list(U_soc_data.keys())\n",
    "easy_S = np.arange(-2.8,2.8,0.1)\n",
    "\n",
    "# Create a list of JAX arrays\n",
    "UK_measured_values = []\n",
    "for p in P:\n",
    "    for w in W:\n",
    "        intensifier = w.name.replace('_',\" \")\n",
    "        predicate = p.name\n",
    "        raw_values = UK_dialogue[((UK_dialogue['intensifier'] == intensifier) & (UK_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "        # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "        z = [int(r*10)+28 for r in raw_values]\n",
    "        x = [0]*len(easy_S)\n",
    "        for i in z:\n",
    "           x[i] += 1\n",
    "        UK_measured_values.append(x)\n",
    "UK_measured_values = np.array(UK_measured_values)\n",
    "\n",
    "@jax.jit\n",
    "def state_prior(s):\n",
    "    return np.exp(-s**2/2) # we assume s is roughly a gaussian distribution\n",
    "\n",
    "@jax.jit\n",
    "def UK_U_soc(intensifier,predicate):\n",
    "    arr = np.array([\n",
    "        [UK_U_soc_data[(w.name.replace('_',\" \"),p.name)] for p in P] \n",
    "        for w in W\n",
    "    ])\n",
    "    return arr[intensifier,predicate]\n",
    "\n",
    "@jax.jit\n",
    "def is_costly(w):\n",
    "    arr = [0, 1, 1, 1, 1, 1]\n",
    "    return np.array(arr)[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def up_down_L(w, s,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5):  # literal likelihood L(w | s)\n",
    "    low_t = np.array([t0,t1,t2,t3,t4,t5])[w]\n",
    "    high_t = np.array([v0,v1, v2, v3, v4, v5])[w]  # Variance parameters for each intensifier\n",
    "    s1 = jax.nn.sigmoid(((s - low_t) * 20).astype(float))\n",
    "    s2 = jax.nn.sigmoid(((high_t - s) * 20).astype(float))\n",
    "    return s1 * s2\n",
    "@memo\n",
    "def up_down_UKL1[s: easy_S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in easy_S, wpp=up_down_L(w, s,t0,t1,t2,t3,t4,t5,v0, v1, v2, v3, v4, v5)), # L(w|s) = literal likelihood,\n",
    "                exp(0.3*inf_term * log(Pr[listener.s == s]+0.0000001) +\n",
    "                0.5*soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                10*cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "def up_down_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    var = params[9:15]  # Extract the variance parameters\n",
    "    P_l1 = np.concatenate([up_down_UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],v0 = var[0], v1=var[1], v2= var[2], v3 = var[3], v4 = var[4], v5 = var[5], p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "def up_down_wrapped_loss(x):\n",
    "    result = up_down_UK_logloss(*tuple(x))  # Unpack the parameters from the tuple\n",
    "    return -result.item()\n",
    "best_params_arr = []\n",
    "best_values_arr = []\n",
    "key = jr.PRNGKey(42)  # Random seed\n",
    "keys = jr.split(key, 100)  # Generate 10 random keys\n",
    "for m in range(100):\n",
    "    initial_val = list(jr.uniform(keys[m], shape=(9,), minval=-3, maxval=3))+ list(jr.uniform(keys[m], shape=(6,), minval=0, maxval= 3))  # Generate 6 random numbers between 0 and 3\n",
    "    print(m,\"initial value:\",initial_val) \n",
    "    for sigma in [0.1, 0.5,1]:\n",
    "        best_params_list = []\n",
    "        best_values_list = []\n",
    "        for _ in range(3):\n",
    "            # Get best solution\n",
    "            es = cma.CMAEvolutionStrategy(\n",
    "                initial_val,    # initial guess\n",
    "                sigma, # sigma\n",
    "                {\"bounds\": [None, None]},\n",
    "            )\n",
    "            es.optimize(up_down_wrapped_loss)\n",
    "            best_params = es.result.xbest\n",
    "            best_value = -es.result.fbest  # Negate to get the original loss value\n",
    "            print(es.stop())\n",
    "            print(\"Best value found:\", best_value)\n",
    "            print(\"Best parameters found:\", best_params)\n",
    "            best_params_list.append(best_params)\n",
    "            best_values_list.append(best_value)\n",
    "        best_params_arr.append(best_params_list)\n",
    "        best_values_arr.append(best_values_list)\n",
    "    # if m%10==0:\n",
    "    #     onp.save(f'cma_best_params.npy', best_params_arr)\n",
    "    #     onp.save(f'cma_best_values.npy', best_values_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import ndtr\n",
    "\n",
    "@jax.jit\n",
    "def ndtr_L(w, s,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5):  # literal likelihood L(w | s)\n",
    "    t = np.array([t0,t1,t2,t3,t4,t5])[w]\n",
    "    var = np.array([v0,v1,v2,v3,v4,v5])[w]  # Variance parameters for each intensifier\n",
    "    return ndtr((s-t)/np.sqrt(var))\n",
    "@memo\n",
    "def ndtr_UKL1[s: easy_S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in easy_S, wpp=ndtr_L(w, s,t0,t1,t2,t3,t4,t5,v0, v1, v2, v3, v4, v5)), # L(w|s) = literal likelihood,\n",
    "                exp(0.3*inf_term * log(Pr[listener.s == s]+0.0000001) +\n",
    "                0.5*soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                10*cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "def ndtr_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    var = params[9:15]  # Extract the variance parameters\n",
    "    P_l1 = np.concatenate([ndtr_UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],v0 = var[0], v1=var[1], v2= var[2], v3 = var[3], v4 = var[4], v5 = var[5], p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "def ndtr_wrapped_loss(x):\n",
    "    result = ndtr_UK_logloss(*tuple(x))  # Unpack the parameters from the tuple\n",
    "    return -result.item()\n",
    "best_params_arr = []\n",
    "best_values_arr = []\n",
    "key = jr.PRNGKey(42)  # Random seed\n",
    "keys = jr.split(key, 100)  # Generate 10 random keys\n",
    "for m in range(100):\n",
    "    initial_val = list(jr.uniform(keys[m], shape=(9,), minval=-3, maxval=3))+ list(jr.uniform(keys[m], shape=(6,), minval=0, maxval= 3))  # Generate 6 random numbers between 0 and 3\n",
    "    print(m,\"initial value:\",initial_val) \n",
    "    for sigma in [0.1, 0.5,1]:\n",
    "        best_params_list = []\n",
    "        best_values_list = []\n",
    "        for _ in range(3):\n",
    "            # Get best solution\n",
    "            es = cma.CMAEvolutionStrategy(\n",
    "                initial_val,    # initial guess\n",
    "                sigma, # sigma\n",
    "                {\"bounds\": [None, None]},\n",
    "            )\n",
    "            es.optimize(ndtr_wrapped_loss)\n",
    "            best_params = es.result.xbest\n",
    "            best_value = -es.result.fbest  # Negate to get the original loss value\n",
    "            print(es.stop())\n",
    "            print(\"Best value found:\", best_value)\n",
    "            print(\"Best parameters found:\", best_params)\n",
    "            best_params_list.append(best_params)\n",
    "            best_values_list.append(best_value)\n",
    "        best_params_arr.append(best_params_list)\n",
    "        best_values_arr.append(best_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def step_L(w, s,t0,t1,t2,t3,t4,t5):  # literal likelihood L(w | s)\n",
    "    t = np.array([t0,t1,t2,t3,t4,t5])[w]\n",
    "    sigt = 1 / (1 + np.exp(-t))\n",
    "    threshold = sigt * 5.6 - 2.8\n",
    "    return jax.lax.cond(\n",
    "        threshold> s,\n",
    "        lambda: epsilon,  # If condition (t > s) is True\n",
    "        lambda: (5.6 - epsilon * (threshold+2.8)) / (2.8 - threshold)  # s and threshold can range from -2.8 to 2.8\n",
    "    )\n",
    "\n",
    "@memo\n",
    "def step_UKL1[s: easy_S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in easy_S, wpp=step_L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "def step_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    P_l1 = np.concatenate([step_UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)**UK_measured_values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussian literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def gaussian_L(w, s,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5):  # literal likelihood L(w | s)\n",
    "    t = np.array([t0,t1,t2,t3,t4,t5])[w]\n",
    "    v = np.array([v0,v1, v2, v3, v4, v5])[w]  # Variance parameters for each intensifier\n",
    "    return np.exp(-(t-s)**2/v)\n",
    "\n",
    "@memo\n",
    "def gaussian_UKL1[s: easy_S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in easy_S, wpp=gaussian_L(w, s,t0,t1,t2,t3,t4,t5,v0, v1, v2, v3, v4, v5)), # L(w|s) = literal likelihood,\n",
    "                exp(0.3*inf_term * log(Pr[listener.s == s]+0.0000001) +\n",
    "                0.5*soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                10*cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "\n",
    "\n",
    "def gaussian_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    var = params[9:15]  # Extract the variance parameters\n",
    "    P_l1 = np.concatenate([gaussian_UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],v0 = var[0], v1=var[1], v2= var[2], v3 = var[3], v4 = var[4], v5 = var[5], p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "\n",
    "def gaussian_wrapped_loss(x):\n",
    "    result = gaussian_UK_logloss(*tuple(x))  # Unpack the parameters from the tuple\n",
    "    return -result.item()\n",
    "best_params_arr = []\n",
    "best_values_arr = []\n",
    "key = jr.PRNGKey(2)  # Random seed\n",
    "keys = jr.split(key, 100)  # Generate 10 random keys\n",
    "for m in range(100):\n",
    "    initial_val = list(jr.uniform(keys[m], shape=(9,), minval=-3, maxval=3))+ list(jr.uniform(keys[m], shape=(6,), minval=0, maxval= 3))  # Generate 6 random numbers between 0 and 3\n",
    "    print(m,\"initial value:\",initial_val) \n",
    "    for sigma in [0.1, 0.5,1]:\n",
    "        best_params_list = []\n",
    "        best_values_list = []\n",
    "        for _ in range(3):\n",
    "            # Get best solution\n",
    "            es = cma.CMAEvolutionStrategy(\n",
    "                initial_val,    # initial guess\n",
    "                sigma, # sigma\n",
    "                {\"bounds\": [None, None]},\n",
    "            )\n",
    "            es.optimize(gaussian_wrapped_loss)\n",
    "            best_params = es.result.xbest\n",
    "            best_value = -es.result.fbest  # Negate to get the original loss value\n",
    "            print(es.stop())\n",
    "            print(\"Best value found:\", best_value)\n",
    "            print(\"Best parameters found:\", best_params)\n",
    "            best_params_list.append(best_params)\n",
    "            best_values_list.append(best_value)\n",
    "        best_params_arr.append(best_params_list)\n",
    "        best_values_arr.append(best_values_list)\n",
    "    # if m%10==0:\n",
    "    #     onp.save(f'cma_best_params.npy', best_params_arr)\n",
    "    #     onp.save(f'cma_best_values.npy', best_values_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting literal listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def literal_UKL1[s: easy_S, w: W](t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=gaussian_L(w, s,t0,t1,t2,t3,t4,t5,v0, v1, v2, v3, v4, v5))\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "\n",
    "def literal_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    var = params[6:12]  # Extract the variance parameters\n",
    "    P_l1 = np.concatenate([literal_UKL1(t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],v0 = var[0], v1=var[1], v2= var[2], v3 = var[3], v4 = var[4], v5 = var[5], p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "\n",
    "def easy_wrapped_loss(x):\n",
    "    result = literal_UK_logloss(*tuple(x))  # Unpack the parameters from the tuple\n",
    "    return -result.item()\n",
    "\n",
    "best_params_arr = []\n",
    "best_values_arr = []\n",
    "key = jr.PRNGKey(42)  # Random seed\n",
    "keys = jr.split(key, 1000)  # Generate 10 random keys\n",
    "for m in range(1000):\n",
    "    initial_val = list(jr.uniform(keys[m], shape=(6,), minval=-3, maxval=3))+ list(jr.uniform(keys[m], shape=(6,), minval=0, maxval= 3))  # Generate 6 random numbers between 0 and 3\n",
    "    print(\"initial value:\",initial_val) \n",
    "    for sigma in [0.1, 0.5,1]:\n",
    "        best_params_list = []\n",
    "        best_values_list = []\n",
    "        for _ in range(3):\n",
    "            # Get best solution\n",
    "            es = cma.CMAEvolutionStrategy(\n",
    "                initial_val,    # initial guess\n",
    "                sigma, # sigma\n",
    "                {\"bounds\": [[None]*6+[0]*6, None],'tolfunhist': 1e-16,'tolflatfitness': 10,'tolfun': 1e-12},\n",
    "            )\n",
    "            es.optimize(easy_wrapped_loss)\n",
    "            best_params = es.result.xbest\n",
    "            best_value = -es.result.fbest  # Negate to get the original loss value\n",
    "            print(es.stop())\n",
    "            print(\"Best value found:\", best_value)\n",
    "            print(\"Best parameters found:\", best_params)\n",
    "            best_params_list.append(best_params)\n",
    "            best_values_list.append(best_value)\n",
    "        best_params_arr.append(best_params_list)\n",
    "        best_values_arr.append(best_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one and two thresholds (defined by thresholds of amplifiers being inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 initial value: [Array(0.18156481, dtype=float32), Array(-1.1198273, dtype=float32), Array(2.4091816, dtype=float32), Array(1.1899974, dtype=float32), Array(0.7083936, dtype=float32), Array(0.73111224, dtype=float32), Array(1.3064804, dtype=float32), Array(-1.7417994, dtype=float32), Array(-2.8596718, dtype=float32), Array(1.5907824, dtype=float32), Array(0.94008636, dtype=float32), Array(2.7045908, dtype=float32), Array(2.0949988, dtype=float32), Array(1.8541968, dtype=float32), Array(1.8655561, dtype=float32)]\n",
      "(6_w,12)-aCMA-ES (mu_w=3.7,w_1=40%) in dimension 15 (seed=688331, Mon Apr 14 20:03:40 2025)\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     12 7.589663085937500e+03 1.0e+00 9.63e-02  3e-02  1e-01 0:00.1\n",
      "    2     24 7.070872070312500e+03 1.1e+00 1.02e-01  3e-02  1e-01 0:00.2\n",
      "    3     36 6.772757324218750e+03 1.2e+00 1.11e-01  3e-02  1e-01 0:00.2\n",
      "  100   1200 5.171135742187500e+03 6.2e+00 1.03e-01  7e-03  1e-01 0:01.4\n",
      "  200   2400 5.170622070312500e+03 2.8e+01 4.11e-03  3e-04  6e-03 0:02.5\n",
      "  300   3600 5.170620117187500e+03 5.1e+01 9.29e-04  6e-05  2e-03 0:03.7\n",
      "  400   4800 5.170620117187500e+03 6.9e+01 4.14e-04  2e-05  7e-04 0:04.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 56\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Get best solution\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     es \u001b[38;5;241m=\u001b[39m cma\u001b[38;5;241m.\u001b[39mCMAEvolutionStrategy(\n\u001b[0;32m     52\u001b[0m         initial_val,    \u001b[38;5;66;03m# initial guess\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         sigma, \u001b[38;5;66;03m# sigma\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m13\u001b[39m\u001b[38;5;241m+\u001b[39m[inf\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m13\u001b[39m\u001b[38;5;241m+\u001b[39m[inf]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m]},\n\u001b[0;32m     55\u001b[0m     )\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_down_wrapped_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m es\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mxbest\n\u001b[0;32m     58\u001b[0m     best_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mes\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mfbest  \u001b[38;5;66;03m# Negate to get the original loss value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\myuhk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cma\\interfaces.py:208\u001b[0m, in \u001b[0;36mOOOptimizer.optimize\u001b[1;34m(self, objective_fct, maxfun, iterations, min_iterations, args, verb_disp, callback, n_jobs, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask()  \u001b[38;5;66;03m# deliver candidate solutions\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# fitvals = [objective_fct(x, *args) for x in X]\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m fitvals \u001b[38;5;241m=\u001b[39m \u001b[43meval_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m cevals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(fitvals)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell(X, fitvals)  \u001b[38;5;66;03m# all the work is done here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\myuhk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\cma\\optimization_tools.py:284\u001b[0m, in \u001b[0;36mEvalParallel2.__call__\u001b[1;34m(self, solutions, fitness_function, args, timeout)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fitness_function` was never given, must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m passed in `__init__` or `__call__`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m solutions]\n\u001b[0;32m    285\u001b[0m warning_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fitness_function` must be a function, not a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `lambda` or an instancemethod, in order to work with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `multiprocessing` under Python 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[33], line 34\u001b[0m, in \u001b[0;36mup_down_wrapped_loss\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mup_down_wrapped_loss\u001b[39m(x):\n\u001b[1;32m---> 34\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mup_down_UK_logloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Unpack the parameters from the tuple\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mresult\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[33], line 31\u001b[0m, in \u001b[0;36mup_down_UK_logloss\u001b[1;34m(*params)\u001b[0m\n\u001b[0;32m     29\u001b[0m soc_term \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m     30\u001b[0m var \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m15\u001b[39m]  \u001b[38;5;66;03m# Extract the variance parameters\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m P_l1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[43mup_down_UKL1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoc_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoc_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt4\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt5\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv4\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv5\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m P],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(P_l1)\u001b[38;5;241m*\u001b[39mUK_measured_values\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32m<string>:105\u001b[0m, in \u001b[0;36m_out_up_down_UKL1\u001b[1;34m(inf_term, soc_term, cost, t0, t1, t2, t3, t4, t5, v0, v1, v2, v3, v4, v5, p, return_aux, return_pandas, return_xarray, return_cost, print_table)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def up_down_L(w, s,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5):  # literal likelihood L(w | s)\n",
    "    low_t = np.array([t0,t1,t2,t3,t4,t5])[w]\n",
    "    high_t = np.array([v0,v1, v2, v3, v4, v5])[w]  # Variance parameters for each intensifier\n",
    "    s1 = jax.nn.sigmoid(((s - low_t) * 20).astype(float))\n",
    "    s2 = jax.nn.sigmoid(((high_t - s) * 20).astype(float))\n",
    "    return s1 * s2\n",
    "@memo\n",
    "def up_down_UKL1[s: easy_S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,v0,v1, v2, v3, v4, v5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in easy_S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in easy_S, wpp=up_down_L(w, s,t0,t1,t2,t3,t4,t5,v0, v1, v2, v3, v4, v5)), # L(w|s) = literal likelihood,\n",
    "                exp(0.3*inf_term * log(Pr[listener.s == s]+0.0000001) +\n",
    "                0.5*soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                10*cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in easy_S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "def up_down_UK_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    var = params[9:15]  # Extract the variance parameters\n",
    "    P_l1 = np.concatenate([up_down_UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],v0 = var[0], v1=var[1], v2= var[2], v3 = var[3], v4 = var[4], v5 = var[5], p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "def up_down_wrapped_loss(x):\n",
    "    result = up_down_UK_logloss(*tuple(x))  # Unpack the parameters from the tuple\n",
    "    return -result.item()\n",
    "best_params_arr = []\n",
    "best_values_arr = []\n",
    "inf = 1000000\n",
    "key = jr.PRNGKey(42)  # Random seed\n",
    "keys = jr.split(key, 100)  # Generate 10 random keys\n",
    "for m in range(100):\n",
    "    initial_val = list(jr.uniform(keys[m], shape=(9,), minval=-3, maxval=3))+ list(jr.uniform(keys[m], shape=(6,), minval=0, maxval= 3))  # Generate 6 random numbers between 0 and 3\n",
    "    print(m,\"initial value:\",initial_val) \n",
    "    initial_val[-2]=inf\n",
    "    initial_val[-1]=inf\n",
    "    for sigma in [0.1, 0.5,1]:\n",
    "        best_params_list = []\n",
    "        best_values_list = []\n",
    "        for _ in range(3):\n",
    "            # Get best solution\n",
    "            es = cma.CMAEvolutionStrategy(\n",
    "                initial_val,    # initial guess\n",
    "                sigma, # sigma\n",
    "                {\"bounds\": [[None]*13+[inf-0.1]*2, [None]*13+[inf]*2]},\n",
    "            )\n",
    "            es.optimize(up_down_wrapped_loss)\n",
    "            best_params = es.result.xbest\n",
    "            best_value = -es.result.fbest  # Negate to get the original loss value\n",
    "            print(es.stop())\n",
    "            print(\"Best value found:\", best_value)\n",
    "            print(\"Best parameters found:\", best_params)\n",
    "            best_params_list.append(best_params)\n",
    "            best_values_list.append(best_value)\n",
    "        best_params_arr.append(best_params_list)\n",
    "        best_values_arr.append(best_values_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
