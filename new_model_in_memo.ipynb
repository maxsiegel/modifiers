{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read in files\n",
    "# Read UK_df.csv as pandas dataframe\n",
    "original_UK_dialogue = pd.read_csv('UK_df.csv')\n",
    "original_UK_politeness = pd.read_csv('UK_direct_df.csv')\n",
    "original_UK_narrator = pd.read_csv('UK_narrator_df.csv')\n",
    "original_US_dialogue = pd.read_csv('US_df.csv')\n",
    "original_US_politeness = pd.read_csv('US_direct_df.csv')\n",
    "original_US_narrator = pd.read_csv('US_narrator_df.csv')\n",
    "dataframes = [original_UK_dialogue, original_UK_politeness, original_UK_narrator, original_US_dialogue, original_US_politeness, original_US_narrator]\n",
    "def elim_outliers(df):\n",
    "    # dropped Unnamed: 0 column\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    filtered_df = df.loc[(df['response'] > 95) | (df['response'] < 5)]\n",
    "    for id in df['person_id'].unique():\n",
    "        if len(filtered_df[filtered_df['person_id'] == id])/len(df[df['person_id'] == id])>0.8:\n",
    "            df.drop(df[df['person_id'] == id].index, inplace=True)\n",
    "    df['predicate Z-score'] = df.groupby(['person_id','predicate'])['response'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    # if has_intensifier = no then change 'intensifier' to 'none'\n",
    "    df.loc[df['has intensifier?'] == 'no', 'intensifier'] = 'none'\n",
    "    return df\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = elim_outliers(dataframes[i])\n",
    "dialogue = pd.concat([dataframes[0], dataframes[3]])\n",
    "politeness = pd.concat([dataframes[1], dataframes[4]])\n",
    "UK_dialogue = dataframes[0]\n",
    "US_dialogue = dataframes[3]\n",
    "UK_politeness = dataframes[1]\n",
    "US_politeness = dataframes[4]\n",
    "\n",
    "# end of reading in data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# compute U_soc (social Utility)\n",
    "U_soc_data = politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "UK_U_soc_data = UK_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "US_U_soc_data = US_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memo import memo\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from enum import IntEnum, auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01\n",
    "infty = 10000000\n",
    "utterences =list(U_soc_data.keys())\n",
    "state_values = np.arange(-2.8,2.8,0.28)\n",
    "S = np.arange(0,20,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define params to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_to_test = np.arange(0,20,1)\n",
    "possible_inf_terms = np.arange(0,5,0.5)\n",
    "possible_soc_terms = np.arange(0,5,0.5)\n",
    "costs = np.arange(0,5,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search (code from demo-rsa.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W(IntEnum):  # utterance space\n",
    "    # borings\n",
    "    none_boring = auto(0)\n",
    "    slightly_boring = auto()\n",
    "    kind_of_boring = auto()\n",
    "    quite_boring = auto()\n",
    "    very_boring = auto()\n",
    "    extremely_boring = auto()\n",
    "    # concerneds\n",
    "    none_concerned = auto()\n",
    "    slightly_concerned = auto()\n",
    "    kind_of_concerned = auto()\n",
    "    quite_concerned = auto()\n",
    "    very_concerned = auto()\n",
    "    extremely_concerned = auto()\n",
    "    # difficults\n",
    "    none_difficult = auto()\n",
    "    slightly_difficult = auto()\n",
    "    kind_of_difficult = auto()\n",
    "    quite_difficult = auto()\n",
    "    very_difficult = auto()\n",
    "    extremely_difficult = auto()\n",
    "    # exhausteds\n",
    "    none_exhausted = auto()\n",
    "    slightly_exhausted = auto()\n",
    "    kind_of_exhausted = auto()\n",
    "    quite_exhausted = auto()\n",
    "    very_exhausted = auto()\n",
    "    extremely_exhausted = auto()\n",
    "    # helpfuls\n",
    "    none_helpful = auto()\n",
    "    slightly_helpful = auto()\n",
    "    kind_of_helpful = auto()\n",
    "    quite_helpful = auto()\n",
    "    very_helpful = auto()\n",
    "    extremely_helpful = auto()\n",
    "    # impressives\n",
    "    none_impressive = auto()\n",
    "    slightly_impressive = auto()\n",
    "    kind_of_impressive = auto()\n",
    "    quite_impressive = auto()\n",
    "    very_impressive = auto()\n",
    "    extremely_impressive = auto()\n",
    "    # understandables\n",
    "    none_understandable = auto()\n",
    "    slightly_understandable = auto()\n",
    "    kind_of_understandable = auto()\n",
    "    quite_understandable = auto()\n",
    "    very_understandable = auto()\n",
    "    extremely_understandable = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the mapping dictionary\n",
    "U_soc_key_map = {w: (' '.join(w.name.split('_')[:-1]), w.name.split('_')[-1]) for w in W}\n",
    "U_soc_array = np.array([U_soc_data[U_soc_key_map[W(i)]] for i in range(len(W))])\n",
    "UK_U_soc_array = np.array([UK_U_soc_data[U_soc_key_map[W(i)]] for i in range(len(W))])\n",
    "US_U_soc_array = np.array([US_U_soc_data[U_soc_key_map[W(i)]] for i in range(len(W))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct measured_values which is an array of 42 arrays where the i'th entry is the values people reported for the i'th utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_literal_semantics = np.array(\n",
    "    [\n",
    "        np.concatenate([np.repeat(np.array([epsilon]),i),np.repeat(np.array([1]),20-i)])\n",
    "        for i in range(20)\n",
    "    ]\n",
    ")\n",
    "@jax.jit\n",
    "def state_prior(s):\n",
    "    return np.float32(np.exp(-state_values[s]**2/2))  # uninformative state prior doesn't matter that it doesn't add up to 1\n",
    "@jax.jit\n",
    "def U_soc(soc):\n",
    "    return U_soc_array[soc]\n",
    "@jax.jit\n",
    "def UK_U_soc(soc):\n",
    "    return UK_U_soc_array[soc]\n",
    "\n",
    "@jax.jit\n",
    "def US_U_soc(soc):\n",
    "    return US_U_soc_array[soc]\n",
    "\n",
    "@jax.jit\n",
    "def is_costly(w):\n",
    "    arr = [0, 1, 1, 1, 1, 1]*7\n",
    "    return np.array(arr)[w]\n",
    "@jax.jit\n",
    "def L(w, s,t0,t1,t2,t3,t4,t5):  # literal likelihood L(w | s)\n",
    "    intensifier_semantics = np.array([  # \"hard semantics\"\n",
    "        possible_literal_semantics[t0],  # none\n",
    "        possible_literal_semantics[t1],  # slightly \n",
    "        possible_literal_semantics[t2],  # kind of\n",
    "        possible_literal_semantics[t3],  # quite\n",
    "        possible_literal_semantics[t4],  # very\n",
    "        possible_literal_semantics[t5],  # extremely\n",
    "    ])\n",
    "    return np.tile(intensifier_semantics, (7, 1))[w, s]\n",
    "\n",
    "@memo\n",
    "def L1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * U_soc(w) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "@memo\n",
    "def UKL1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * UK_U_soc(w) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "@memo\n",
    "def USL1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * US_U_soc(w) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of JAX arrays\n",
    "UK_measured_values = []\n",
    "for w in W:\n",
    "    intensifier, predicate = U_soc_key_map[w]   \n",
    "    raw_values = UK_dialogue[((UK_dialogue['intensifier'] == intensifier) & (UK_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "    # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "    z = [int(r/0.28)+10 for r in raw_values]\n",
    "    x = [0]*len(S)\n",
    "    for i in z:\n",
    "        x[i] += 1\n",
    "    UK_measured_values.append(x)\n",
    "UK_measured_values = np.array(UK_measured_values)\n",
    "# Create a list of JAX arrays\n",
    "US_measured_values = []\n",
    "for w in W:\n",
    "    intensifier, predicate = U_soc_key_map[w]   \n",
    "    raw_values = US_dialogue[((US_dialogue['intensifier'] == intensifier) & (US_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "    # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "    z = [int(r/0.28)+10 for r in raw_values]\n",
    "    x = [0]*len(S)\n",
    "    for i in z:\n",
    "        x[i] += 1\n",
    "    US_measured_values.append(x)\n",
    "US_measured_values = np.array(US_measured_values)\n",
    "def compute_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. log_likelihood\n",
    "    P_l1 = L1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5]) # note this should be P(s|w)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T),np.sum(np.log(P_l1)*US_measured_values.T)\n",
    "def UK_soc_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. log_likelihood\n",
    "    P_l1 = UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5]) # note this should be P(s|w)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "def US_soc_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. log_likelihood\n",
    "    P_l1 = USL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5]) # note this should be P(s|w)\n",
    "    return np.sum(np.log(P_l1)*US_measured_values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param_combos = len(costs)*len(possible_soc_terms)*len(possible_inf_terms)*len(theta_to_test)**2\n",
    "in_repeat = total_param_combos//len(theta_to_test)\n",
    "out_repeat = 1\n",
    "# first_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# second_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# third_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# fourth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "fifth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(theta_to_test)\n",
    "in_repeat = in_repeat//len(theta_to_test)\n",
    "sixth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(theta_to_test)\n",
    "in_repeat = in_repeat//len(costs)\n",
    "seventh_costs = np.repeat(np.tile(costs, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(costs)\n",
    "in_repeat = in_repeat//len(possible_inf_terms)\n",
    "eighth_infs = np.repeat(np.tile(possible_inf_terms, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(possible_inf_terms)\n",
    "in_repeat = in_repeat//len(possible_soc_terms)\n",
    "ninth_socs = np.repeat(np.tile(possible_soc_terms, in_repeat),out_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t3 \u001b[38;5;129;01min\u001b[39;00m theta_to_test:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t4 \u001b[38;5;129;01min\u001b[39;00m theta_to_test:\n\u001b[1;32m----> 7\u001b[0m         output\u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUK_soc_logloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt4\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfifth_thetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43msixth_thetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseventh_costs\u001b[49m\u001b[43m,\u001b[49m\u001b[43meighth_infs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mninth_socs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_until_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         all_output\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t1,t2,t3)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compute UK specific result\n",
    "all_output = []\n",
    "for t1 in theta_to_test:\n",
    "    for t2 in theta_to_test:\n",
    "        for t3 in theta_to_test:\n",
    "            for t4 in theta_to_test:\n",
    "                output= jax.vmap(UK_soc_logloss,in_axes=(None,)*4+(0,)*5)(t1,t2,t3,t4,fifth_thetas,sixth_thetas,seventh_costs,eighth_infs,ninth_socs).block_until_ready()\n",
    "                all_output.append(output)\n",
    "            print(t1,t2,t3)\n",
    "# save all_output to a file\n",
    "all_output = np.array(all_output).flatten()\n",
    "np.save('UK_specific_soc_feb26.npy', all_output)\n",
    "del all_output\n",
    "# compute US specific result\n",
    "all_output = []\n",
    "for t1 in theta_to_test:\n",
    "    for t2 in theta_to_test:\n",
    "        for t3 in theta_to_test:\n",
    "            for t4 in theta_to_test:\n",
    "                output= jax.vmap(US_soc_logloss,in_axes=(None,)*4+(0,)*5)(t1,t2,t3,t4,fifth_thetas,sixth_thetas,seventh_costs,eighth_infs,ninth_socs).block_until_ready()\n",
    "                all_output.append(output)\n",
    "            print(t1,t2,t3)\n",
    "# save all_output to a file\n",
    "all_output = np.array(all_output).flatten()\n",
    "np.save('US_specific_soc_feb26.npy', all_output)\n",
    "del all_output\n",
    "# compute UK/US specific result\n",
    "UK_all_output = []\n",
    "US_all_output = []\n",
    "for t1 in theta_to_test:\n",
    "    for t2 in theta_to_test:\n",
    "        for t3 in theta_to_test:\n",
    "            for t4 in theta_to_test:\n",
    "                UK_output,US_output = jax.vmap(compute_logloss,in_axes=(None,)*4+(0,)*5)(t1,t2,t3,t4,fifth_thetas,sixth_thetas,seventh_costs,eighth_infs,ninth_socs).block_until_ready()\n",
    "                UK_all_output.append(UK_output)\n",
    "                US_all_output.append(US_output)\n",
    "            print(t1,t2,t3)\n",
    "# save all_output to a file\n",
    "UK_all_output = np.array(UK_all_output).flatten()\n",
    "np.save('UK_standard_feb26.npy', UK_all_output)\n",
    "del UK_all_output\n",
    "US_all_output = np.array(US_all_output).flatten()\n",
    "np.save('US_standard_feb26.npy', US_all_output)\n",
    "del US_all_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
