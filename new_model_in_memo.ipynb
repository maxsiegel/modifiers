{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read in files\n",
    "# Read UK_df.csv as pandas dataframe\n",
    "original_UK_dialogue = pd.read_csv('UK_df.csv')\n",
    "original_UK_politeness = pd.read_csv('UK_direct_df.csv')\n",
    "original_UK_narrator = pd.read_csv('UK_narrator_df.csv')\n",
    "original_US_dialogue = pd.read_csv('US_df.csv')\n",
    "original_US_politeness = pd.read_csv('US_direct_df.csv')\n",
    "original_US_narrator = pd.read_csv('US_narrator_df.csv')\n",
    "dataframes = [original_UK_dialogue, original_UK_politeness, original_UK_narrator, original_US_dialogue, original_US_politeness, original_US_narrator]\n",
    "def elim_outliers(df):\n",
    "    # dropped Unnamed: 0 column\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    filtered_df = df.loc[(df['response'] > 95) | (df['response'] < 5)]\n",
    "    for id in df['person_id'].unique():\n",
    "        if len(filtered_df[filtered_df['person_id'] == id])/len(df[df['person_id'] == id])>0.8:\n",
    "            df.drop(df[df['person_id'] == id].index, inplace=True)\n",
    "    df['predicate Z-score'] = df.groupby(['person_id','predicate'])['response'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    # if has_intensifier = no then change 'intensifier' to 'none'\n",
    "    df.loc[df['has intensifier?'] == 'no', 'intensifier'] = 'none'\n",
    "    return df\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = elim_outliers(dataframes[i])\n",
    "dialogue = pd.concat([dataframes[0], dataframes[3]])\n",
    "politeness = pd.concat([dataframes[1], dataframes[4]])\n",
    "UK_dialogue = dataframes[0]\n",
    "US_dialogue = dataframes[3]\n",
    "UK_politeness = dataframes[1]\n",
    "US_politeness = dataframes[4]\n",
    "\n",
    "# end of reading in data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# compute U_soc (social Utility)\n",
    "U_soc_data = politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "UK_U_soc_data = UK_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()\n",
    "US_U_soc_data = US_politeness.groupby(['intensifier','predicate'])['predicate Z-score'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memo import memo\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from enum import IntEnum, auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01\n",
    "infty = 10000000\n",
    "utterences =list(U_soc_data.keys())\n",
    "state_values = np.arange(-2.8,2.8,0.28)\n",
    "S = np.arange(0,20,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define params to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_to_test = np.arange(0,20,1)\n",
    "possible_inf_terms = np.arange(0,1.5,0.3)\n",
    "possible_soc_terms = np.arange(0,1.5,0.3)\n",
    "costs = np.arange(0,4,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W(IntEnum):  # utterance space\n",
    "    # intensifiers\n",
    "    none = auto(0)\n",
    "    slightly= auto()\n",
    "    kind_of = auto()\n",
    "    quite = auto()\n",
    "    very= auto()\n",
    "    extremely= auto()\n",
    "class P(IntEnum):\n",
    "    # predicates\n",
    "    boring = auto(0)\n",
    "    concerned = auto()\n",
    "    difficult = auto()\n",
    "    exhausted = auto()\n",
    "    helpful = auto()\n",
    "    impressive = auto()\n",
    "    understandable = auto()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct measured_values which is an array of 42 arrays where the i'th entry is the values people reported for the i'th utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_literal_semantics = np.array(\n",
    "    [\n",
    "        np.concatenate([np.repeat(np.array([epsilon]),i),np.repeat(np.array([1]),20-i)])\n",
    "        for i in range(20)\n",
    "    ]\n",
    ")\n",
    "@jax.jit\n",
    "def state_prior(s):\n",
    "    return np.float32(np.exp(-state_values[s]**2/2))  # uninformative state prior doesn't matter that it doesn't add up to 1\n",
    "@jax.jit\n",
    "def U_soc(intensifier,predicate):\n",
    "    arr = np.array([\n",
    "        [U_soc_data[(w.name.replace('_',\" \"),p.name)] for p in P] \n",
    "        for w in W\n",
    "    ])\n",
    "    return arr[intensifier,predicate]\n",
    "@jax.jit\n",
    "def UK_U_soc(intensifier,predicate):\n",
    "    arr = np.array([\n",
    "        [UK_U_soc_data[(w.name.replace('_',\" \"),p.name)] for p in P] \n",
    "        for w in W\n",
    "    ])\n",
    "    return arr[intensifier,predicate]\n",
    "@jax.jit\n",
    "def US_U_soc(intensifier,predicate):\n",
    "    arr = np.array([\n",
    "        [US_U_soc_data[(w.name.replace('_',\" \"),p.name)] for p in P] \n",
    "        for w in W\n",
    "    ])\n",
    "    return arr[intensifier,predicate]\n",
    "@jax.jit\n",
    "def is_costly(w):\n",
    "    arr = [0, 1, 1, 1, 1, 1]\n",
    "    return np.array(arr)[w]\n",
    "\n",
    "@jax.jit\n",
    "def L(w, s,t0,t1,t2,t3,t4,t5):  # literal likelihood L(w | s)\n",
    "    intensifier_semantics = np.array([  # \"hard semantics\"\n",
    "        possible_literal_semantics[t0],  # none\n",
    "        possible_literal_semantics[t1],  # slightly \n",
    "        possible_literal_semantics[t2],  # kind of\n",
    "        possible_literal_semantics[t3],  # quite\n",
    "        possible_literal_semantics[t4],  # very\n",
    "        possible_literal_semantics[t5],  # extremely\n",
    "    ])\n",
    "    return intensifier_semantics[w,s]  # \"hard semantics\"\n",
    "\n",
    "@memo\n",
    "def L1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * U_soc(w,p) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "@memo\n",
    "def UKL1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * UK_U_soc(w,p) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]\n",
    "@memo\n",
    "def USL1[s: S, w: W](inf_term, soc_term, cost,t0,t1,t2,t3,t4,t5,p):\n",
    "    listener: thinks[\n",
    "        speaker: given(s in S, wpp=state_prior(s)),\n",
    "        speaker: chooses(w in W, wpp=\n",
    "            imagine[\n",
    "                listener: knows(w),\n",
    "                listener: chooses(s in S, wpp=L(w, s,t0,t1,t2,t3,t4,t5)) ,\n",
    "                exp(inf_term * log(Pr[listener.s == s]) + \n",
    "                soc_term * US_U_soc(w,p) - # U_soc = listener's EU\n",
    "                cost*is_costly(w)) # U_inf = listener's surprisal       \n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    listener: observes[speaker.w] is w\n",
    "    listener: chooses(s in S, wpp=Pr[speaker.s == s])\n",
    "    return Pr[listener.s == s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of JAX arrays\n",
    "UK_measured_values = []\n",
    "for p in P:\n",
    "    for w in W:\n",
    "        intensifier = w.name.replace('_',\" \")\n",
    "        predicate = p.name\n",
    "        raw_values = UK_dialogue[((UK_dialogue['intensifier'] == intensifier) & (UK_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "        # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "        z = [int(r/0.28)+10 for r in raw_values]\n",
    "        x = [0]*len(S)\n",
    "        for i in z:\n",
    "            x[i] += 1\n",
    "        UK_measured_values.append(x)\n",
    "UK_measured_values = np.array(UK_measured_values)\n",
    "# Create a list of JAX arrays\n",
    "US_measured_values = []\n",
    "for p in P:\n",
    "    for w in W:\n",
    "        intensifier = w.name.replace('_',\" \")\n",
    "        predicate = p.name \n",
    "        raw_values = US_dialogue[((US_dialogue['intensifier'] == intensifier) & (US_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "        # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "        z = [int(r/0.28)+10 for r in raw_values]\n",
    "        x = [0]*len(S)\n",
    "        for i in z:\n",
    "            x[i] += 1\n",
    "        US_measured_values.append(x)\n",
    "US_measured_values = np.array(US_measured_values)\n",
    "def compute_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. log_likelihood\n",
    "    P_l1 = np.concatenate([L1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T),np.sum(np.log(P_l1)*US_measured_values.T)\n",
    "def UK_soc_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. log_likelihood\n",
    "    for p in P:\n",
    "        for w in W:\n",
    "            intensifier = w.name.replace('_',\" \")\n",
    "            predicate = p.name\n",
    "            raw_values = UK_dialogue[((UK_dialogue['intensifier'] == intensifier) & (UK_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "            # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "            z = [int(r/0.28)+10 for r in raw_values]\n",
    "            x = [0]*len(S)\n",
    "            for i in z:\n",
    "                x[i] += 1\n",
    "    P_l1 = np.concatenate([UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*UK_measured_values.T)\n",
    "def US_soc_logloss(*params):\n",
    "    thetas = params[:6]\n",
    "    cost = params[6]\n",
    "    inf_term = params[7]\n",
    "    soc_term = params[8]\n",
    "    # compute fit e.g. \n",
    "    for p in P:\n",
    "        for w in W:\n",
    "            intensifier = w.name.replace('_',\" \")\n",
    "            predicate = p.name\n",
    "            raw_values = US_dialogue[((US_dialogue['intensifier'] == intensifier) & (US_dialogue['predicate'] == predicate))]['predicate Z-score'].values\n",
    "            # measured_values.append(np.array([int(r/0.28)+10 for r in raw_values]))\n",
    "            z = [int(r/0.28)+10 for r in raw_values]\n",
    "            x = [0]*len(S)\n",
    "            for i in z:\n",
    "                x[i] += 1\n",
    "    P_l1 = np.concatenate([USL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis = 1)\n",
    "    return np.sum(np.log(P_l1)*US_measured_values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sum rows of UK_measured_values which are same mod 6\n",
    "# import numpy as np\n",
    "# # plot histogram of arr\n",
    "# print(np.sum(UK_measured_values[1::6],axis=0))\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(state_values,np.sum(US_measured_values[1::6],axis=0), marker = 'o',color='r', label=\"US\")\n",
    "# plt.plot(state_values,np.sum(UK_measured_values[1::6],axis=0), marker = 'o',color='b', label=\"UK\")\n",
    "# plt.legend()\n",
    "# plt.xlabel('z-score threshold')\n",
    "# plt.ylabel('count for dialogue scenario')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param_combos = len(costs)*len(possible_soc_terms)*len(possible_inf_terms)*len(theta_to_test)**3\n",
    "in_repeat = total_param_combos//len(theta_to_test)\n",
    "out_repeat = 1\n",
    "# first_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# second_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# third_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# fourth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "# fifth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "# out_repeat = out_repeat*len(theta_to_test)\n",
    "# in_repeat = in_repeat//len(theta_to_test)\n",
    "sixth_thetas = np.repeat(np.tile(theta_to_test, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(theta_to_test)\n",
    "in_repeat = in_repeat//len(costs)\n",
    "seventh_costs = np.repeat(np.tile(costs, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(costs)\n",
    "in_repeat = in_repeat//len(possible_inf_terms)\n",
    "eighth_infs = np.repeat(np.tile(possible_inf_terms, in_repeat),out_repeat)\n",
    "out_repeat = out_repeat*len(possible_inf_terms)\n",
    "in_repeat = in_repeat//len(possible_soc_terms)\n",
    "ninth_socs = np.repeat(np.tile(possible_soc_terms, in_repeat),out_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t4 \u001b[38;5;129;01min\u001b[39;00m theta_to_test:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t5 \u001b[38;5;129;01min\u001b[39;00m theta_to_test:\n\u001b[1;32m----> 8\u001b[0m         output\u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUK_soc_logloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt4\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt5\u001b[49m\u001b[43m,\u001b[49m\u001b[43msixth_thetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseventh_costs\u001b[49m\u001b[43m,\u001b[49m\u001b[43meighth_infs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mninth_socs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_until_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         all_output\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t1,t2,t3)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compute UK specific result\n",
    "for t1 in theta_to_test:\n",
    "    all_output = []\n",
    "    for t2 in theta_to_test:\n",
    "        for t3 in theta_to_test:\n",
    "            for t4 in theta_to_test:\n",
    "                for t5 in theta_to_test:\n",
    "                    output= jax.vmap(UK_soc_logloss,in_axes=(None,)*5+(0,)*4)(t1,t2,t3,t4,t5,sixth_thetas,seventh_costs,eighth_infs,ninth_socs).block_until_ready()\n",
    "                    all_output.append(output)\n",
    "            print(t1,t2,t3)\n",
    "    all_output = np.array(all_output).flatten()\n",
    "    np.save(f'UK_specific_soc_march5_{t1}.npy', all_output)\n",
    "    del all_output\n",
    "# compute US specific result\n",
    "for t1 in theta_to_test:\n",
    "    all_output = []\n",
    "    for t2 in theta_to_test:\n",
    "        for t3 in theta_to_test:\n",
    "            for t4 in theta_to_test:\n",
    "                for t5 in theta_to_test:\n",
    "                    output= jax.vmap(US_soc_logloss,in_axes=(None,)*5+(0,)*4)(t1,t2,t3,t4,t5,sixth_thetas,seventh_costs,eighth_infs,ninth_socs).block_until_ready()\n",
    "                    all_output.append(output)\n",
    "            print(t1,t2,t3)\n",
    "    all_output = np.array(all_output).flatten()\n",
    "    np.save(f'US_specific_soc_march5_{t1}.npy', all_output)\n",
    "    del all_output\n",
    "# compute UK/US specific result\n",
    "# UK_all_output = []\n",
    "# US_all_output = []\n",
    "# for t1 in theta_to_test:\n",
    "#     for t2 in theta_to_test:\n",
    "#         for t3 in theta_to_test:\n",
    "#             UK_output,US_output = jax.vmap(compute_logloss,in_axes=(None,)*3+(0,)*6)(t1,t2,t3,fourth_thetas,fifth_thetas,sixth_thetas,seventh_costs,eighth_infs,ninth_socs)\n",
    "#             UK_all_output.append(UK_output)\n",
    "#             US_all_output.append(US_output)\n",
    "#         print(t1,t2)\n",
    "# # save all_output to a file\n",
    "# UK_all_output = np.array(UK_all_output).flatten()\n",
    "# np.save(f'UK_standard_march5.npy', UK_all_output)\n",
    "# del UK_all_output\n",
    "# US_all_output = np.array(US_all_output).flatten()\n",
    "# np.save(f'US_standard_march5.npy', US_all_output)\n",
    "# del US_all_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce graph from ideal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for utterence in W:\n",
    "#     print(U_soc_key_map[utterence])\n",
    "#     # Compute weighted mean\n",
    "#     weighted_mean = np.average(state_values, weights=P_l1[:,utterence])\n",
    "\n",
    "#     # Compute weighted variance\n",
    "#     variance = np.average((state_values - weighted_mean) ** 2, weights=P_l1[:,utterence])\n",
    "#     # Compute weighted standard deviation\n",
    "#     std_dev = np.sqrt(variance)\n",
    "# plot values next to eachother of each of values in new_US[('difference','mean')].keys()\n",
    "# so x lables are new_US[('difference','mean')].keys() which are predicate intensifier pairs\n",
    "# y values are new_US[('difference','mean')].values() and new_UK[('difference','mean')].values()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# US_soc_logloss(7,19,5,7,7,12,0,0.5,0)\n",
    "cost = 0\n",
    "inf_term = 0.6\n",
    "soc_term = 0\n",
    "thetas = [8,0,6,8,8,10] # these should be integers\n",
    "P_l1 = np.concat([USL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis=1)# note this should be P(s|w)\n",
    "us_means = [np.average(state_values, weights=P_l1[:,u]) for u in range(len(P_l1[0]))]\n",
    "us_std = [np.average((state_values - us_means[u]) ** 2, weights=P_l1[:,u]) for u in range(len(P_l1[0]))]\n",
    "us_var = [us_std[u]**0.5 for u in W]\n",
    "\n",
    "# specify ideal parameters for UK (coarser search)\n",
    "thetas = [8,0,6,6,12,12] # these should be integers\n",
    "cost = 0.6\n",
    "inf_term = 0.6\n",
    "soc_term = 2.4\n",
    "# this is for UK version\n",
    "P_l1 = np.concat([UKL1(inf_term=inf_term, soc_term=soc_term, cost=cost, t0 = thetas[0],t1=thetas[1], t2= thetas[2], t3 = thetas[3], t4 = thetas[4], t5 = thetas[5],p=p) for p in P],axis=1)# note this should be P(s|w)\n",
    "uk_means = [np.average(state_values, weights=P_l1[:,u]) for u in range(len(P_l1[0]))]\n",
    "uk_std = [np.average((state_values - uk_means[u]) ** 2, weights=P_l1[:,u]) for u in range(len(P_l1[0]))]\n",
    "uk_var = [uk_std[u]**0.5 for u in W]\n",
    "# Plotting\n",
    "# convert range(48) to np array\n",
    "x = np.array(range(42))  # X positions for bars\n",
    "width = 0.35  # Width of the bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars_uk = ax.bar(x + width/2, uk_means, width, label='UK')\n",
    "bars_us = ax.bar(x - width/2, us_means, width, label='US')\n",
    "\n",
    "# # Add error bars\n",
    "# ax.errorbar(x - width/2, us_means, us_std, fmt='none', ecolor='black', capsize=2)\n",
    "# ax.errorbar(x + width/2, uk_means, uk_std, fmt='none', ecolor='black', capsize=2)\n",
    "# remove the numbers on bar\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Intensifier')\n",
    "ax.set_ylabel('Mean Difference')\n",
    "ax.set_title('Mean Difference by Intensifier')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{p.name}+{utterence.name}' for p in P for utterence in W] , rotation=45, ha='right')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
